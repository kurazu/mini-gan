{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q3h36oGKPlTz"
      },
      "outputs": [],
      "source": [
        "import os.path\n",
        "import math\n",
        "\n",
        "import click\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "from functools import partial\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "uCgy1g_iPlT-"
      },
      "outputs": [],
      "source": [
        "input_file = 'gs://flickr-faces-mini/resized.tar'\n",
        "# input_dir = 'gs://flickr-faces-mini/resized'\n",
        "input_dir = './resized'\n",
        "output_dir = './output'\n",
        "img_size = 28\n",
        "noise_dim = 100\n",
        "batch_size = 256\n",
        "shuffle_buffer_size = batch_size * 10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp {input_file} . && tar xf resized.tar"
      ],
      "metadata": {
        "id": "RdCCHu9mV7oO",
        "outputId": "a842fce4-40b1-4e1c-fc48-62da64d3aac2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://flickr-faces-mini/resized.tar...\n",
            "- [1 files][ 99.0 MiB/ 99.0 MiB]                                                \n",
            "Operation completed over 1 objects/99.0 MiB.                                     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "EoZbqmXiPlUD"
      },
      "outputs": [],
      "source": [
        "def process_path(file_path: tf.Tensor, img_size: int) -> tf.Tensor:\n",
        "    file_contents = tf.io.read_file(file_path)\n",
        "    img = tf.io.decode_png(file_contents, channels=1)\n",
        "    normalized_image = (tf.cast(img, tf.float32) - 127.5) / 127.5\n",
        "    return normalized_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "ft_Aw1bTPlUJ"
      },
      "outputs": [],
      "source": [
        "files_pattern = os.path.join(input_dir, \"*\", \"*.png\")\n",
        "train_ds = (\n",
        "    tf.data.Dataset.list_files(files_pattern, shuffle=False)\n",
        "    .map(\n",
        "        partial(process_path, img_size=img_size),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
        "    )\n",
        "    .cache()\n",
        "    .shuffle(shuffle_buffer_size)\n",
        "    .batch(batch_size)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zGwlx0YPlUR",
        "outputId": "c7da371a-cca4-463c-c092-eab409527dfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "264\n"
          ]
        }
      ],
      "source": [
        "cardinality = tf.data.experimental.cardinality(train_ds).numpy().tolist()\n",
        "print(cardinality)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "MQ2V3FdePlUZ"
      },
      "outputs": [],
      "source": [
        "# plt.figure(figsize=(10, 10))\n",
        "# for batch in train_ds.take(1):\n",
        "#     batch_size, *_ = batch.shape\n",
        "#     for i in range(batch_size):\n",
        "#       plt.subplot(math.ceil(batch_size / 16), 16, i + 1)\n",
        "#       plt.imshow(batch[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "#       plt.axis('off')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "dpVZUv4uPlUn"
      },
      "outputs": [],
      "source": [
        "def generate_and_save_images(\n",
        "    output_dir: str, model: tf.keras.Model, epoch: int, test_input: tf.Tensor\n",
        ") -> None:\n",
        "    # Notice `training` is set to False.\n",
        "    # This is so all layers run in inference mode (batchnorm).\n",
        "    predictions = model(test_input, training=False)\n",
        "\n",
        "    fig = plt.figure(figsize=(4, 4))\n",
        "    batch_size, *_ = predictions.shape\n",
        "    for i in range(batch_size):\n",
        "        plt.subplot(4, 4, i + 1)\n",
        "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap=\"gray\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.savefig(os.path.join(output_dir, f\"image_at_epoch_{epoch:04d}.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "2TyM3QHSPlU0"
      },
      "outputs": [],
      "source": [
        "def make_generator_model(*, img_size: int, noise_dim: int) -> tf.keras.Model:\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(\n",
        "        tf.keras.layers.Dense(\n",
        "            7 * 7 * 256, use_bias=False, input_shape=(noise_dim,)\n",
        "        )\n",
        "    )\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "    model.add(tf.keras.layers.Reshape((7, 7, 256)))\n",
        "    assert model.output_shape == (\n",
        "        None,\n",
        "        7,\n",
        "        7,\n",
        "        256,\n",
        "    )  # Note: None is the batch size\n",
        "\n",
        "    model.add(\n",
        "        tf.keras.layers.Conv2DTranspose(\n",
        "            128, (5, 5), strides=(1, 1), padding=\"same\", use_bias=False\n",
        "        )\n",
        "    )\n",
        "    assert model.output_shape == (None, 7, 7, 128)\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "    model.add(\n",
        "        tf.keras.layers.Conv2DTranspose(\n",
        "            64, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False\n",
        "        )\n",
        "    )\n",
        "    assert model.output_shape == (None, 14, 14, 64)\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "    model.add(\n",
        "        tf.keras.layers.Conv2DTranspose(\n",
        "            1,\n",
        "            (5, 5),\n",
        "            strides=(2, 2),\n",
        "            padding=\"same\",\n",
        "            use_bias=False,\n",
        "            activation=\"tanh\",\n",
        "        )\n",
        "    )\n",
        "    assert model.output_shape == (None, img_size, img_size, 1)\n",
        "\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "gzIuAsjuPlVF"
      },
      "outputs": [],
      "source": [
        "def make_discriminator_model(*, img_size: int) -> tf.keras.Model:\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(\n",
        "        tf.keras.layers.Conv2D(\n",
        "            64,\n",
        "            (5, 5),\n",
        "            strides=(2, 2),\n",
        "            padding=\"same\",\n",
        "            input_shape=[img_size, img_size, 1],\n",
        "        )\n",
        "    )\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "    model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "    model.add(\n",
        "        tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding=\"same\")\n",
        "    )\n",
        "    model.add(tf.keras.layers.LeakyReLU())\n",
        "    model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.list_physical_devices()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UIhBM8gR9fR",
        "outputId": "64977514-ad73-4a01-c591-cf95fce03321"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
              " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6t_ifrIPlVL",
        "outputId": "30b892cb-433f-4e85-ce28-5de4e17ce2cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 12544)             1254400   \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 12544)            50176     \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_15 (LeakyReLU)  (None, 12544)             0         \n",
            "                                                                 \n",
            " reshape_3 (Reshape)         (None, 7, 7, 256)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_9 (Conv2DT  (None, 7, 7, 128)        819200    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 7, 7, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_16 (LeakyReLU)  (None, 7, 7, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_10 (Conv2D  (None, 14, 14, 64)       204800    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 14, 14, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_17 (LeakyReLU)  (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_11 (Conv2D  (None, 28, 28, 1)        1600      \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,330,944\n",
            "Trainable params: 2,305,472\n",
            "Non-trainable params: 25,472\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "with tf.distribute.MirroredStrategy().scope():\n",
        "    generator = make_generator_model(img_size=img_size, noise_dim=noise_dim)\n",
        "    discriminator = make_discriminator_model(img_size=img_size)\n",
        "\n",
        "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "    generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "    discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(\n",
        "    generator_optimizer=generator_optimizer,\n",
        "    discriminator_optimizer=discriminator_optimizer,\n",
        "    generator=generator,\n",
        "    discriminator=discriminator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "oIMBqnUEPlVR"
      },
      "outputs": [],
      "source": [
        "def discriminator_loss(\n",
        "    cross_entropy, real_output: tf.Tensor, fake_output: tf.Tensor\n",
        ") -> tf.Tensor:\n",
        "\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "def generator_loss(cross_entropy, fake_output: tf.Tensor) -> tf.Tensor:\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8s7pzDKPlVW",
        "outputId": "1c4620eb-81c6-4649-98c4-d92f7790a156"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded checkoint from ./output/ckpt-305\n"
          ]
        }
      ],
      "source": [
        "latest_checkpoint_dir = tf.train.latest_checkpoint(output_dir)\n",
        "if latest_checkpoint_dir:\n",
        "  _, epoch_str = latest_checkpoint_dir.rsplit(\"-\", 1)\n",
        "  start_epoch = int(epoch_str)\n",
        "  _ = checkpoint.restore(latest_checkpoint_dir)\n",
        "  print(\"Loaded checkoint from\", latest_checkpoint_dir)\n",
        "else:\n",
        "  print(\"No checkpoint to load\")\n",
        "  start_epoch = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "hzzWvkyXPlVZ"
      },
      "outputs": [],
      "source": [
        "num_examples_to_generate = 16\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "qPVIGj_aPlVb"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(images: tf.Tensor) -> None:\n",
        "    noise = tf.random.normal([batch_size, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(cross_entropy, fake_output)\n",
        "        disc_loss = discriminator_loss(\n",
        "            cross_entropy, real_output, fake_output\n",
        "        )\n",
        "\n",
        "        gradients_of_generator = gen_tape.gradient(\n",
        "            gen_loss, generator.trainable_variables\n",
        "        )\n",
        "        gradients_of_discriminator = disc_tape.gradient(\n",
        "            disc_loss, discriminator.trainable_variables\n",
        "        )\n",
        "\n",
        "        generator_optimizer.apply_gradients(\n",
        "            zip(gradients_of_generator, generator.trainable_variables)\n",
        "        )\n",
        "        discriminator_optimizer.apply_gradients(\n",
        "            zip(\n",
        "                gradients_of_discriminator,\n",
        "                discriminator.trainable_variables,\n",
        "            )\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "RnB90vOFPlVm"
      },
      "outputs": [],
      "source": [
        "def train(dataset, epochs):\n",
        "    generate_and_save_images(output_dir, generator, start_epoch, seed)\n",
        "\n",
        "    for epoch in range(start_epoch, epochs + start_epoch):\n",
        "        for image_batch in tqdm(\n",
        "            dataset,\n",
        "            total=cardinality,\n",
        "            desc=f\"Epoch {epoch + 1}\",\n",
        "        ):\n",
        "            train_step(image_batch)\n",
        "\n",
        "        # Produce images for the GIF as you go\n",
        "        generate_and_save_images(output_dir, generator, epoch + 1, seed)\n",
        "\n",
        "        checkpoint.save(file_prefix=os.path.join(output_dir, \"ckpt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npHVEJ_IPlVp",
        "outputId": "ba7460ea-b647-425c-d482-dbfcb0d551a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 306: 100%|██████████| 264/264 [00:20<00:00, 12.89it/s]\n",
            "Epoch 307: 100%|██████████| 264/264 [00:15<00:00, 17.36it/s]\n",
            "Epoch 308: 100%|██████████| 264/264 [00:15<00:00, 17.11it/s]\n",
            "Epoch 309: 100%|██████████| 264/264 [00:15<00:00, 16.79it/s]\n",
            "Epoch 310: 100%|██████████| 264/264 [00:15<00:00, 16.72it/s]\n",
            "Epoch 311: 100%|██████████| 264/264 [00:15<00:00, 16.72it/s]\n",
            "Epoch 312: 100%|██████████| 264/264 [00:15<00:00, 16.66it/s]\n",
            "Epoch 313: 100%|██████████| 264/264 [00:15<00:00, 16.59it/s]\n",
            "Epoch 314: 100%|██████████| 264/264 [00:15<00:00, 16.63it/s]\n",
            "Epoch 315: 100%|██████████| 264/264 [00:15<00:00, 16.64it/s]\n",
            "Epoch 316: 100%|██████████| 264/264 [00:15<00:00, 16.68it/s]\n",
            "Epoch 317: 100%|██████████| 264/264 [00:15<00:00, 16.69it/s]\n",
            "Epoch 318: 100%|██████████| 264/264 [00:15<00:00, 16.66it/s]\n",
            "Epoch 319: 100%|██████████| 264/264 [00:15<00:00, 16.67it/s]\n",
            "Epoch 320: 100%|██████████| 264/264 [00:15<00:00, 16.68it/s]\n",
            "Epoch 321: 100%|██████████| 264/264 [00:15<00:00, 16.65it/s]\n",
            "Epoch 322: 100%|██████████| 264/264 [00:15<00:00, 16.62it/s]\n",
            "Epoch 323: 100%|██████████| 264/264 [00:15<00:00, 16.62it/s]\n",
            "Epoch 324: 100%|██████████| 264/264 [00:15<00:00, 16.64it/s]\n",
            "Epoch 325: 100%|██████████| 264/264 [00:15<00:00, 16.64it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "  \n",
            "Epoch 326: 100%|██████████| 264/264 [00:15<00:00, 16.61it/s]\n",
            "Epoch 327: 100%|██████████| 264/264 [00:15<00:00, 16.60it/s]\n",
            "Epoch 328: 100%|██████████| 264/264 [00:15<00:00, 16.59it/s]\n",
            "Epoch 329: 100%|██████████| 264/264 [00:15<00:00, 16.63it/s]\n",
            "Epoch 330: 100%|██████████| 264/264 [00:20<00:00, 12.89it/s]\n",
            "Epoch 331: 100%|██████████| 264/264 [00:16<00:00, 16.50it/s]\n",
            "Epoch 332: 100%|██████████| 264/264 [00:16<00:00, 16.47it/s]\n",
            "Epoch 333: 100%|██████████| 264/264 [00:15<00:00, 16.60it/s]\n",
            "Epoch 334: 100%|██████████| 264/264 [00:15<00:00, 16.70it/s]\n",
            "Epoch 335: 100%|██████████| 264/264 [00:15<00:00, 16.69it/s]\n",
            "Epoch 336: 100%|██████████| 264/264 [00:15<00:00, 16.63it/s]\n",
            "Epoch 337: 100%|██████████| 264/264 [00:15<00:00, 16.62it/s]\n",
            "Epoch 338: 100%|██████████| 264/264 [00:15<00:00, 16.58it/s]\n",
            "Epoch 339: 100%|██████████| 264/264 [00:15<00:00, 16.61it/s]\n",
            "Epoch 340: 100%|██████████| 264/264 [00:15<00:00, 16.64it/s]\n",
            "Epoch 341: 100%|██████████| 264/264 [00:15<00:00, 16.60it/s]\n",
            "Epoch 342: 100%|██████████| 264/264 [00:15<00:00, 16.61it/s]\n",
            "Epoch 343: 100%|██████████| 264/264 [00:15<00:00, 16.60it/s]\n",
            "Epoch 344: 100%|██████████| 264/264 [00:15<00:00, 16.62it/s]\n",
            "Epoch 345: 100%|██████████| 264/264 [00:15<00:00, 16.62it/s]\n",
            "Epoch 346: 100%|██████████| 264/264 [00:15<00:00, 16.60it/s]\n",
            "Epoch 347: 100%|██████████| 264/264 [00:15<00:00, 16.65it/s]\n",
            "Epoch 348: 100%|██████████| 264/264 [00:15<00:00, 16.63it/s]\n",
            "Epoch 349: 100%|██████████| 264/264 [00:15<00:00, 16.68it/s]\n",
            "Epoch 350: 100%|██████████| 264/264 [00:15<00:00, 16.64it/s]\n",
            "Epoch 351: 100%|██████████| 264/264 [00:15<00:00, 16.68it/s]\n",
            "Epoch 352: 100%|██████████| 264/264 [00:15<00:00, 16.65it/s]\n",
            "Epoch 353: 100%|██████████| 264/264 [00:15<00:00, 16.65it/s]\n",
            "Epoch 354: 100%|██████████| 264/264 [00:15<00:00, 16.62it/s]\n",
            "Epoch 355: 100%|██████████| 264/264 [00:15<00:00, 16.62it/s]\n",
            "Epoch 356: 100%|██████████| 264/264 [00:15<00:00, 16.62it/s]\n",
            "Epoch 357: 100%|██████████| 264/264 [00:15<00:00, 16.59it/s]\n",
            "Epoch 358: 100%|██████████| 264/264 [00:15<00:00, 16.62it/s]\n",
            "Epoch 359: 100%|██████████| 264/264 [00:15<00:00, 16.58it/s]\n",
            "Epoch 360: 100%|██████████| 264/264 [00:15<00:00, 16.61it/s]\n",
            "Epoch 361: 100%|██████████| 264/264 [00:15<00:00, 16.60it/s]\n",
            "Epoch 362: 100%|██████████| 264/264 [00:15<00:00, 16.61it/s]\n",
            "Epoch 363: 100%|██████████| 264/264 [00:15<00:00, 16.58it/s]\n",
            "Epoch 364: 100%|██████████| 264/264 [00:15<00:00, 16.59it/s]\n",
            "Epoch 365: 100%|██████████| 264/264 [00:15<00:00, 16.62it/s]\n",
            "Epoch 366: 100%|██████████| 264/264 [00:15<00:00, 16.63it/s]\n",
            "Epoch 367: 100%|██████████| 264/264 [00:15<00:00, 16.60it/s]\n",
            "Epoch 368: 100%|██████████| 264/264 [00:15<00:00, 16.62it/s]\n",
            "Epoch 369: 100%|██████████| 264/264 [00:15<00:00, 16.63it/s]\n",
            "Epoch 370: 100%|██████████| 264/264 [00:20<00:00, 12.90it/s]\n",
            "Epoch 371: 100%|██████████| 264/264 [00:15<00:00, 16.58it/s]\n",
            "Epoch 372: 100%|██████████| 264/264 [00:16<00:00, 16.46it/s]\n",
            "Epoch 373: 100%|██████████| 264/264 [00:15<00:00, 16.58it/s]\n",
            "Epoch 374: 100%|██████████| 264/264 [00:15<00:00, 16.63it/s]\n",
            "Epoch 375: 100%|██████████| 264/264 [00:15<00:00, 16.68it/s]\n",
            "Epoch 376: 100%|██████████| 264/264 [00:15<00:00, 16.65it/s]\n",
            "Epoch 377: 100%|██████████| 264/264 [00:15<00:00, 16.63it/s]\n",
            "Epoch 378: 100%|██████████| 264/264 [00:15<00:00, 16.62it/s]\n",
            "Epoch 379: 100%|██████████| 264/264 [00:15<00:00, 16.59it/s]\n",
            "Epoch 380: 100%|██████████| 264/264 [00:15<00:00, 16.60it/s]\n",
            "Epoch 381: 100%|██████████| 264/264 [00:15<00:00, 16.64it/s]\n",
            "Epoch 382: 100%|██████████| 264/264 [00:15<00:00, 16.66it/s]\n",
            "Epoch 383: 100%|██████████| 264/264 [00:15<00:00, 16.63it/s]\n",
            "Epoch 384:  98%|█████████▊| 259/264 [00:15<00:00, 16.60it/s]"
          ]
        }
      ],
      "source": [
        "train(train_ds, 100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3PTaJ08gQm2L"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.10 ('python-3.8.10': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "9a81e8d0a4d83858f0e8f1109855ac6e9f8dcfc3e7832b92eba8c17268ec9cc5"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}