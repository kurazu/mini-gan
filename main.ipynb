{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import math\n",
    "\n",
    "import click\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from returns.curry import partial\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'gs://flickr-faces-mini/resized'\n",
    "output_dir = './output'\n",
    "img_size = 28\n",
    "noise_dim = 100\n",
    "batch_size = 256\n",
    "shuffle_buffer_size = batch_size * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(file_path: tf.Tensor, img_size: int) -> tf.Tensor:\n",
    "    file_contents = tf.io.read_file(file_path)\n",
    "    img = tf.io.decode_png(file_contents, channels=1)\n",
    "    normalized_image = (tf.cast(img, tf.float32) - 127.5) / 127.5\n",
    "    return normalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-22 15:08:10.661675: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-08-22 15:08:10.661717: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-22 15:08:10.661748: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (lapdog): /proc/driver/nvidia/version does not exist\n",
      "2022-08-22 15:08:10.663914: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-22 15:08:10.691944: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
     ]
    }
   ],
   "source": [
    "files_pattern = os.path.join(input_dir, \"*\", \"*.png\")\n",
    "train_ds = (\n",
    "    tf.data.Dataset.list_files(files_pattern, shuffle=False)\n",
    "    .map(\n",
    "        partial(process_path, img_size=img_size),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    )\n",
    "    .cache()\n",
    "    .shuffle(shuffle_buffer_size)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n"
     ]
    }
   ],
   "source": [
    "cardinality = tf.data.experimental.cardinality(train_ds).numpy().tolist()\n",
    "print(cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 10))\n",
    "# for batch in train_ds.take(1):\n",
    "#     batch_size, *_ = batch.shape\n",
    "#     for i in range(batch_size):\n",
    "#       plt.subplot(math.ceil(batch_size / 16), 16, i + 1)\n",
    "#       plt.imshow(batch[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "#       plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(\n",
    "    output_dir: str, model: tf.keras.Model, epoch: int, test_input: tf.Tensor\n",
    ") -> None:\n",
    "    # Notice `training` is set to False.\n",
    "    # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    batch_size, *_ = predictions.shape\n",
    "    for i in range(batch_size):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.savefig(os.path.join(output_dir, f\"image_at_epoch_{epoch:04d}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model(*, img_size: int, noise_dim: int) -> tf.keras.Model:\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(\n",
    "            7 * 7 * 256, use_bias=False, input_shape=(noise_dim,)\n",
    "        )\n",
    "    )\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    model.add(tf.keras.layers.Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (\n",
    "        None,\n",
    "        7,\n",
    "        7,\n",
    "        256,\n",
    "    )  # Note: None is the batch size\n",
    "\n",
    "    model.add(\n",
    "        tf.keras.layers.Conv2DTranspose(\n",
    "            128, (5, 5), strides=(1, 1), padding=\"same\", use_bias=False\n",
    "        )\n",
    "    )\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    model.add(\n",
    "        tf.keras.layers.Conv2DTranspose(\n",
    "            64, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False\n",
    "        )\n",
    "    )\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    model.add(\n",
    "        tf.keras.layers.Conv2DTranspose(\n",
    "            1,\n",
    "            (5, 5),\n",
    "            strides=(2, 2),\n",
    "            padding=\"same\",\n",
    "            use_bias=False,\n",
    "            activation=\"tanh\",\n",
    "        )\n",
    "    )\n",
    "    assert model.output_shape == (None, img_size, img_size, 1)\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model(*, img_size: int) -> tf.keras.Model:\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(\n",
    "        tf.keras.layers.Conv2D(\n",
    "            64,\n",
    "            (5, 5),\n",
    "            strides=(2, 2),\n",
    "            padding=\"same\",\n",
    "            input_shape=[img_size, img_size, 1],\n",
    "        )\n",
    "    )\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "    model.add(\n",
    "        tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding=\"same\")\n",
    "    )\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 12544)             1254400   \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 12544)            50176     \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 12544)             0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 7, 7, 256)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 7, 7, 128)        819200    \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 7, 7, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 14, 14, 64)       204800    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 14, 14, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 28, 28, 1)        1600      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,330,944\n",
      "Trainable params: 2,305,472\n",
      "Non-trainable params: 25,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = make_generator_model(img_size=img_size, noise_dim=noise_dim)\n",
    "discriminator = make_discriminator_model(img_size=img_size)\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "    generator_optimizer=generator_optimizer,\n",
    "    discriminator_optimizer=discriminator_optimizer,\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(\n",
    "    cross_entropy, real_output: tf.Tensor, fake_output: tf.Tensor\n",
    ") -> tf.Tensor:\n",
    "\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def generator_loss(cross_entropy, fake_output: tf.Tensor) -> tf.Tensor:\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkoint from ./output/ckpt-103\n"
     ]
    }
   ],
   "source": [
    "latest_checkpoint_dir = tf.train.latest_checkpoint(output_dir)\n",
    "_, epoch_str = latest_checkpoint_dir.rsplit(\"-\", 1)\n",
    "start_epoch = int(epoch_str)\n",
    "_ = checkpoint.restore(latest_checkpoint_dir)\n",
    "print(\"Loaded checkoint from\", latest_checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples_to_generate = 16\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images: tf.Tensor) -> None:\n",
    "    noise = tf.random.normal([batch_size, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(cross_entropy, fake_output)\n",
    "        disc_loss = discriminator_loss(\n",
    "            cross_entropy, real_output, fake_output\n",
    "        )\n",
    "\n",
    "        gradients_of_generator = gen_tape.gradient(\n",
    "            gen_loss, generator.trainable_variables\n",
    "        )\n",
    "        gradients_of_discriminator = disc_tape.gradient(\n",
    "            disc_loss, discriminator.trainable_variables\n",
    "        )\n",
    "\n",
    "        generator_optimizer.apply_gradients(\n",
    "            zip(gradients_of_generator, generator.trainable_variables)\n",
    "        )\n",
    "        discriminator_optimizer.apply_gradients(\n",
    "            zip(\n",
    "                gradients_of_discriminator,\n",
    "                discriminator.trainable_variables,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    generate_and_save_images(output_dir, generator, start_epoch, seed)\n",
    "\n",
    "    for epoch in range(start_epoch, epochs + start_epoch):\n",
    "        for image_batch in tqdm(\n",
    "            dataset,\n",
    "            total=cardinality,\n",
    "            desc=f\"Epoch {epoch + 1}\",\n",
    "        ):\n",
    "            train_step(image_batch)\n",
    "\n",
    "        # Produce images for the GIF as you go\n",
    "        generate_and_save_images(output_dir, generator, epoch + 1, seed)\n",
    "\n",
    "        checkpoint.save(file_prefix=os.path.join(output_dir, \"ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 104:  70%|███████   | 186/264 [07:23<03:05,  2.38s/it]"
     ]
    }
   ],
   "source": [
    "train(train_ds, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('python-3.8.10': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a81e8d0a4d83858f0e8f1109855ac6e9f8dcfc3e7832b92eba8c17268ec9cc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
